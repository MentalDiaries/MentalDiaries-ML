# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rl4wf1nTT_0Bj6wZXYPkKrWASySAxISK
"""

from wordcloud import WordCloud
import numpy as np 
import pandas as pd
from sklearn.model_selection import train_test_split
!pip install neattext
import neattext.functions as nfx
import matplotlib.pyplot as plt
import plotly.express as plx
from sklearn.metrics import classification_report
import keras
from keras.layers import Embedding,Dense,LSTM,Bidirectional,GlobalMaxPooling1D,Input,Dropout
from keras.callbacks import EarlyStopping,ReduceLROnPlateau
from keras.models import Sequential
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from tqdm import tqdm
import seaborn as sns
import pickle
import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

data=pd.read_csv("/content/drive/My Drive/archive/Suicide_Detection.csv")
data.head()

from google.colab import drive
drive.mount('/content/drive')

data['class'].value_counts()

data['class'].value_counts().index.values

train_data,test_data=train_test_split(data,test_size=0.2,random_state=10)

train_data['class'].value_counts().index.values

plx.bar(train_data,x=train_data['class'].value_counts().index.values,
        y=train_data['class'].value_counts(),color=['Suicide','Not Suicide'])

plt.figure(figsize=(12,10))
plt.pie(train_data['class'].value_counts(),startangle=90,colors=['#00dddf','#000fbb'],
        autopct='%0.2f%%',labels=['suicide','Not Suicide'])
plt.title('SUICIDE OR NOT ?',fontdict={'size':20})
plt.show()

def clean_text(text):
    text_length=[]
    cleaned_text=[]
    for sent in tqdm(text):
        sent=sent.lower()
        sent=nfx.remove_special_characters(sent)
        sent=nfx.remove_stopwords(sent)
#         sent=nfx.remove_shortwords(sent)
        text_length.append(len(sent.split()))
        cleaned_text.append(sent)
    return cleaned_text,text_length

cleaned_train_text,train_text_length=clean_text(train_data.text)
cleaned_test_text,test_text_length=clean_text(test_data.text)

plt.figure(figsize=(12,10))
sns.distplot(train_text_length)
# plt.axis([-10,100,0,0.03])
plt.show()

tokenizer=Tokenizer()
tokenizer.fit_on_texts(cleaned_train_text)

word_freq=pd.DataFrame(tokenizer.word_counts.items(),columns=['word','count']).sort_values(by='count',ascending=False)

plt.figure(figsize=(15,20))
sns.barplot(x='count',y='word',data=word_freq.iloc[:50])
plt.show()

feature_names=word_freq['word'].values
wc=WordCloud(max_words=400)
wc.generate(' '.join(word for word in feature_names[500:3500] ))
plt.figure(figsize=(20,15))
plt.axis('off')
plt.imshow(wc)



train_text_seq=tokenizer.texts_to_sequences(cleaned_train_text)
train_text_pad=pad_sequences(train_text_seq,maxlen=50)


test_text_seq=tokenizer.texts_to_sequences(cleaned_test_text)
test_text_pad=pad_sequences(test_text_seq,maxlen=50)

lbl_target=LabelEncoder()
train_output=lbl_target.fit_transform(train_data['class'])
test_output=lbl_target.transform(test_data['class'])



lbl_target=LabelEncoder()
train_output=lbl_target.fit_transform(train_data['class'])
test_output=lbl_target.transform(test_data['class'])

# glove_embedding={}
with open('/content/drive/My Drive/archive/glove.840B.300d.pkl', 'rb') as glEmb:
    glove_embedding = pickle.load(glEmb)

v=len(tokenizer.word_index)

embedding_matrix=np.zeros((v+1,300), dtype=float)
for word,idx in tokenizer.word_index.items():
    embedding_vector=glove_embedding.get(word)
    if embedding_vector is not None:
        embedding_matrix[idx]=embedding_vector

early_stop=EarlyStopping(patience=5)
reducelr=ReduceLROnPlateau(patience=3)



model=Sequential()
model.add(Input(shape=(50,)))
model.add(Embedding(v+1,300,weights=[embedding_matrix],trainable=False))
model.add(LSTM(20,return_sequences=True))
model.add(GlobalMaxPooling1D())
#     model.add(Dropout(0.3))
model.add(Dense(256,activation='relu'))
#     model.add(Dropout(0.2))
model.add(Dense(1,activation='sigmoid'))
model.compile(optimizer=tf.keras.optimizers.SGD(0.1,momentum=0.09),loss='binary_crossentropy',metrics=['accuracy'])
#model.compile(
 #    optimizer = tf.keras.optimizers.SGD(0.1,momentum=0.09),
  #   loss = 'binary_crossentropy',
   #  metrics = ["accuracy"]
    #)

r=model.fit(train_text_pad,train_output,validation_data=(test_text_pad,test_output),
            epochs=20,batch_size=256,callbacks=[early_stop,reducelr])



plt.figure(figsize=(10,8))
plt.plot(r.history['accuracy'])
plt.plot(r.history['val_accuracy'])
plt.title('ACCURACY CURVE',fontdict={'size':20})
plt.show()

plt.figure(figsize=(8,10))
plt.plot(r.history['loss'])
plt.plot(r.history['val_loss'])
plt.title('LOSS CURVE',fontdict={'size':20})
plt.show()

print('TESTING DATA CLASSIFICATION REPORT \n \n')

score, acc = model.evaluate(test_text_pad,test_output,verbose=2, batch_size= 256)
print('TRAINING DATA CLASSIFICATION REPORT \n \n',score,acc)



pp

print('TESTING DATA CLASSIFICATION REPORT \n \n')
print(classification_report(test_output,model.predict(test_text_pad)==1,
                            target_names=lbl_target.inverse_transform([0,1])))
print('\n n')
print('TRAINING DATA CLASSIFICATION REPORT \n \n')
print(classification_report(train_output,model.predict(train_text_pad)==1,
                           target_names=lbl_target.inverse_transform([0,1])))

a=['I am feeling  depressed and im going to suicide']
def pre(a:str):
  a_clean,a_length = clean_text(a)
  a_clean_seq=tokenizer.texts_to_sequences(a_clean)
  a_clean_pad=pad_sequences(a_clean_seq,maxlen=50)
  print('\n')
  print(model.predict(a_clean_pad))
pre(a)

filename = 'finalized_model.sav'
pickle.dump(model, open(filename, 'wb'))